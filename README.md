# optimal-control-and-reinforcement-learning

# 1 researcher
[Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/) Robotic Artificial Intelligence and Learning Lab(RAIL) @ UC Berkeley

[Zac Manchester](https://www.linkedin.cn/incareer/in/zacmanchester) The Robotic Exploration Lab @ Carnegie Mellon University 

[Marco Pavone](https://stanfordasl.github.io/people/prof-marco-pavone/) Autonomous Systems Laboratory @ Stanford

[Marco Hutter](https://rsl.ethz.ch/the-lab/people/person-detail.MTIxOTEx.TGlzdC8yNDQxLC0xNDI1MTk1NzM1.html) Robotic Systems Lab @ ETH zurich

# 2 organization

## 2.1 [Robotic Artificial Intelligence and Learning Lab](http://rail.eecs.berkeley.edu/)(RAIL)
> Welcome to the RAIL lab website! Our research focus is to enable machines to exhibit flexible and adaptable behavior, acquired autonomously through learning. To that end, we work on learning algorithms, robotics, and computer vision.

[website](https://rail.eecs.berkeley.edu/index.html), [people](https://rail.eecs.berkeley.edu/people.html), [publication](https://rail.eecs.berkeley.edu/publications.html), [project](), [code](https://rail.eecs.berkeley.edu/code.html)

## 2.2 [The Robotic Exploration Lab @ Carnegie Mellon University](http://roboticexplorationlab.org/)
The Robotic Exploration Lab in The Robotics Institute at Carnegie Mellon University conducts research in control, motion planning, and navigation for robotic systems that explore our planet and our universe.

[website](https://rexlab.ri.cmu.edu/), [people](https://rexlab.ri.cmu.edu/people.html), [publication](https://rexlab.ri.cmu.edu/publications.html), [project](), [code](https://github.com/RoboticExplorationLab)

## 2.3 [Stanford Autonomous Systems Lab](https://stanfordasl.github.io/)

> The Autonomous Systems Lab (ASL) develops methodologies for the analysis, design, and control of autonomous systems, with a particular emphasis on large-scale robotic networks and autonomous aerospace vehicles. The lab, comprised of 18 researchers, combines expertise from control theory, robotics, optimization, and operations research to develop the theoretical foundations for networked autonomous systems operating in uncertain, rapidly-changing, and potentially adversarial environments. Our work has been recognized with several awards, including a Presidential Early Career Award for Scientists and Engineers.

[website](https://stanfordasl.github.io/), [people](https://stanfordasl.github.io/people/), [publication](https://stanfordasl.github.io/publications/), [project](https://stanfordasl.github.io/projects/), [code](https://github.com/StanfordASL)

## 2.4 [Robotic Systems Lab](https://rsl.ethz.ch/education-students/student-projects0/available-projects.html)

> The Robotic Systems Lab investigates the development of machines and their intelligence to operate in rough and challenging environments. With a large focus on robots with arms and legs, our research includes novel actuation methods for advanced dynamic interaction, innovative designs for increased system mobility and versatility, and new control and optimization algorithms for locomotion and manipulation. In search of clever solutions, we take inspiration from humans and animals with the goal to improve the skills and autonomy of complex robotic systems to make them applicable in various real-world scenarios.

[website](https://rsl.ethz.ch/), [people](https://rsl.ethz.ch/the-lab/people.html), [publication](https://rsl.ethz.ch/publications-sources/publications.html), [project](https://rsl.ethz.ch/education-students/student-projects0/available-projects.html), [code](https://github.com/leggedrobotics/)


## 2.5 [System Control and Optimization Laboratory](https://www.syscop.de/teaching)

> The research focus of the laboratory is on optimization and control with a current application focus on robotic, mechatronic and renewable energy systems. The core area of expertise are embedded optimization algorithms, i.e. methods for real-time optimization on embedded platforms, with a focus on nonlinear systems. The work of the group spans from dynamic system modelling and optimal control problem formulations to open-source software development and real-world control implementations. The groupâ€™s interdisciplinary work is located between numerical mathematics, computer science, and control engineering.

[website](https://www.syscop.de/), [people](https://www.syscop.de/people), [publication](https://www.syscop.de/publications), [project](https://www.syscop.de/research/projects), [code](https://www.syscop.de/research/software)


# 3 book
## 3.1 [Reinforcement learning and optimal control](https://web.mit.edu/dimitrib/www/RLbook.html)
> Dimitri P. Bertsekas

> The purpose of the book is to consider large and challenging multistage decision problems, which can be solved in principle by dynamic programming and optimal control, but their exact solution is computationally intractable. We discuss solution methods that rely on approximations to produce suboptimal policies with adequate performance. These methods are collectively referred to as reinforcement learning, and also by alternative names such as approximate dynamic programming, and neuro-dynamic programming.

> Our subject has benefited enormously from the interplay of ideas from optimal control and from artificial intelligence. One of the aims of this monograph is to explore the common boundary between these two fields and to form a bridge that is accessible by workers with background in either field.

> The mathematical style of the book is somewhat different from the author's dynamic programming books, and the neuro-dynamic programming monograph, written jointly with John Tsitsiklis. We rely more on intuitive explanations and less on proof-based insights. Still we provide a rigorous short account of the theory of finite and infinite horizon dynamic programming, and some basic approximation methods, in an appendix. For this we require a modest mathematical background: calculus, elementary probability, and a minimal use of matrix-vector algebra.

> The methods of this book have been successful in practice, and often spectacularly so, as evidenced by recent amazing accomplishments in the games of chess and Go. However, across a wide range of problems, their performance properties may be less than solid. This is a reflection of the state of the art in the field: there are no methods that are guaranteed to work for all or even most problems, but there are enough methods to try on a given challenging problem with a reasonable chance that one or more of them will be successful in the end. Accordingly, we have aimed to present a broad range of methods that are based on sound principles, and to provide intuition into their properties, even when these properties do not include a solid performance guarantee. Hopefully, with enough exploration with some of these methods and their variations, the reader will be able to address adequately his/her own problem.

# 4 courses

## 4.1 [AA 203: Optimal and Learning-Based Control](https://stanfordasl.github.io/aa203/)

> Optimal control solution techniques for systems with known and unknown dynamics. Dynamic programming, Hamilton-Jacobi reachability, and direct and indirect methods for trajectory optimization. Introduction to model predictive control. Adaptive control, model-based and model-free reinforcement learning, and connections between modern reinforcement learning and fundamental optimal control ideas.

## 4.2 [Reinforcement learning and optimal control for robotics (ROB-GY 6323)](http://bulletin.engineering.nyu.edu/preview_course_nopop.php?catoid=15&coid=38033)
> What kind of movements should a robot perform in order to walk, jump or manipulate objects? Can it compute optimal behaviors online? Can it learn this directly from trial and error? This course will introduce modern methods for robotics movement generation based on numerical optimal control and reinforcement learning. It will cover fundamental topics in numerical optimal control (Bellman equations, differential dynamic programming, model predictive control) and reinforcement learning (actor-critic algorithms, model-based reinforcement learning, deep reinforcement learning) applied to robotics. It will also contain hands-on exercises for real robotic applications such as walking and jumping, object manipulation or acrobatic drones. Recommended background in at least one of the following: linear systems; robotics; machine learning; convex optimization; programming (python).

## 4.3 [Optimal Control and Reinforcement Learning @CMU](https://www.cs.cmu.edu/~cga/dynopt/description.html)
> This course surveys the use of optimization to design behavior. We will explore ways to represent policies including hand-designed parametric functions, basis functions, tables, and trajectory libraries. We will also explore algorithms to create policies including parameter optimization and trajectory optimization (first and second order gradient methods, sequential quadratic programming, random search methods, evolutionary algorithms, etc.). We will discuss how to handle the discrepancy between models used to create policies and the actual system being controlled (evaluation and robustness issues). The course will combine lectures, student-presented material, and projects. The goal of this course will be to help participants find the most effective methods for their problems.

## 4.4 [Model Predictive Control and Reinforcement Learning](https://www.syscop.de/teaching/ss2021/model-predictive-control-and-reinforcement-learning)

> This block course of 8 days duration is intended for master and PhD students from engineering, computer science, mathematics, physics, and other mathematical sciences. The aim is that participants understand the main concepts of model predictive control (MPC) and reinforcement learning (RL) as well the similarities and differences between the two approaches. In hands-on exercises and project work they learn to apply the methods to practical optimal control problems from science and engineering. 


## appendix 1

> https://djrusso.github.io/Dynamic-Optimization-Course/

> https://idsc.ethz.ch/education/lectures/optimal-control.html

