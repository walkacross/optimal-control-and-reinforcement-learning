# optimal-control-and-reinforcement-learning


# 1.book
# 2.courses

## 2.1 [AA 203: Optimal and Learning-Based Control](https://stanfordasl.github.io/aa203/)

> Optimal control solution techniques for systems with known and unknown dynamics. Dynamic programming, Hamilton-Jacobi reachability, and direct and indirect methods for trajectory optimization. Introduction to model predictive control. Adaptive control, model-based and model-free reinforcement learning, and connections between modern reinforcement learning and fundamental optimal control ideas.



## 2.2 [Reinforcement learning and optimal control for robotics (ROB-GY 6323)](http://bulletin.engineering.nyu.edu/preview_course_nopop.php?catoid=15&coid=38033)
> What kind of movements should a robot perform in order to walk, jump or manipulate objects? Can it compute optimal behaviors online? Can it learn this directly from trial and error? This course will introduce modern methods for robotics movement generation based on numerical optimal control and reinforcement learning. It will cover fundamental topics in numerical optimal control (Bellman equations, differential dynamic programming, model predictive control) and reinforcement learning (actor-critic algorithms, model-based reinforcement learning, deep reinforcement learning) applied to robotics. It will also contain hands-on exercises for real robotic applications such as walking and jumping, object manipulation or acrobatic drones. Recommended background in at least one of the following: linear systems; robotics; machine learning; convex optimization; programming (python).

## 2.3 [Model Predictive Control and Reinforcement Learning](https://www.syscop.de/teaching/ss2021/model-predictive-control-and-reinforcement-learning)

> This block course of 8 days duration is intended for master and PhD students from engineering, computer science, mathematics, physics, and other mathematical sciences. The aim is that participants understand the main concepts of model predictive control (MPC) and reinforcement learning (RL) as well the similarities and differences between the two approaches. In hands-on exercises and project work they learn to apply the methods to practical optimal control problems from science and engineering. 

# 3.researcher

# 4.organization

## 4.1 [Stanford Autonomous Systems Lab](https://stanfordasl.github.io/)

> The Autonomous Systems Lab (ASL) develops methodologies for the analysis, design, and control of autonomous systems, with a particular emphasis on large-scale robotic networks and autonomous aerospace vehicles. The lab, comprised of 18 researchers, combines expertise from control theory, robotics, optimization, and operations research to develop the theoretical foundations for networked autonomous systems operating in uncertain, rapidly-changing, and potentially adversarial environments. Our work has been recognized with several awards, including a Presidential Early Career Award for Scientists and Engineers.

https://github.com/StanfordASL

## 4.2 [System control and optimization laboratory](https://www.syscop.de/teaching)

> The research focus of the laboratory is on optimization and control with a current application focus on robotic, mechatronic and renewable energy systems. The core area of expertise are embedded optimization algorithms, i.e. methods for real-time optimization on embedded platforms, with a focus on nonlinear systems. The work of the group spans from dynamic system modelling and optimal control problem formulations to open-source software development and real-world control implementations. The groupâ€™s interdisciplinary work is located between numerical mathematics, computer science, and control engineering.
